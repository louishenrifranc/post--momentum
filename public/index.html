<!DOCTYPE html><html><head><meta charset="utf-8">
<meta name="viewport" content="width=768, initial-scale=1">
<script src="http://distill.pub/template.v1.js"></script>
<script type="text/front-matter">
  title: Attention and Augmented Recurrent Neural Networks
  description: A visual overview of neural attention, and the powerful extensions of neural networks being built on top of it.
  authors:
    - Julien Roy: https://github.com/julienroy13 
    - Felix G. Harvey: https://github.com/XefPatterson/INF8225_Project 
    - Louis Henri Franc: https://github.com/louishenrifranc
  affiliations:
      - Polytechnique Montreal: polymtl.ca
      - Polytechnique Montreal: polymtl.ca
      - Polytechnique Montreal: polymtl.ca
</script>
<!-- Katex -->
<script src="assets/lib/auto-render.min.js"></script>
<script src="assets/lib/katex.min.js"></script>
<link rel="stylesheet" href="assets/lib/katex.min.css">
<link rel="stylesheet" type="text/css" href="assets/widgets.css">
<link rel="stylesheet" type="text/css" href="assets/main.css">
<!-- Required -->
<script src="assets/lib/lib.js"></script>
<script src="assets/utils.js"></script>
<script>
var renderQueue = [];

function renderMath(elem) {
    renderMathInElement(
        elem, {
            delimiters: [{
                left: "$$",
                right: "$$",
                display: true
            }, {
                left: "$",
                right: "$",
                display: false
            }, ]
        }
    );
}

var deleteQueue = [];

function renderLoading(figure) {
    var loadingScreen = figure.append("svg")
        .style("width", figure.style("width"))
        .style("height", figure.style("height"))
        .style("position", "absolute")
        .style("top", "0px")
        .style("left", "0px")
        .style("background", "white")
        .style("border", "0px dashed #DDD")
        .style("opacity", 1)

    return function(callback) {
        loadingScreen
            .remove()
    }

}
</script>

<style>
#previews figcaption {
  text-align: center;
}
</style>


    </head><body>
        <dt-article class="centered">
            <h1>Seq2Seq Model with Attention and Chatbot</h1>
            <dt-byline></dt-byline>
            <p>
                </p><p>
                    Recurrent Recurrent Networks (RNN) are a special kind of artificial neural networks that are specialized in extracting information from sequences. Unlike simple feedfoward neural networks, a neuron's activation is also dependent from its previous activations. This allows the model to capture correlations between the different inputs in the sequence.
                </p>
                <p>
                    However, this type of architecture can be very challenging to train, partly because it is much more prone to exploding and vanishing gradients
                    <dt-cite key="bengio1994difficult"></dt-cite>. These problems can be overcome by choosing more stable activation functions like <i>ReLU</i> or <i>tanh</i> and by using more sophisticated cells like <i>LSTM</i> and <i>GRU</i>, which involve more parameters and computations than the vanilla RNN cell, but are designed to avoid vanishing gradients and capture long range dependencies
                    <dt-cite key="Chung2014GRUvsLSTM"></dt-cite>.
                </p>
                <p>
                    In the recent years, RNNs along with the mentioned strategies have been able to achieve <i>state of the art</i> level of performance on many challenging tasks like conversational models
                    <dt-cite key="vinyals2015conversational"></dt-cite>, machine translation
                    <dt-cite key="Bahdanau2014translation"></dt-cite> and automatic summarization
                    <dt-cite key="Rush2015summarization"></dt-cite>. In this project, we implement a sequence to sequence (Seq2Seq) model using two independent RNNs, an encoder and a decoder. We use the <i>Cornell Movie</i> dialog dataset
                    <dt-cite key="CornellDataset"></dt-cite> to train a short text answering model similar to the work of <i>Shang et al.</i><dt-cite key="shang2015shortText"></dt-cite>. We also explore the use of similar models on custom datasets of Facebook-Messenger conversation histories.
                </p>
                <hr>
                <h2>
                    A bit of theory
                </h2>
                <h3>
                    Recurrent Neural Networks
                </h3>
                <p class="toRender">
                    In a regular RNN, at
                    <i>
                        time-step
                    </i>
                    $t$, the cell state $h_t$ is computed based on its own input and the cell state $h_t$ that encapsulates some information from the precedent inputs : $$h_t = f(W^{hx}x_t + W^{hh}h_{t-1})$$
                </p>
                <p class="toRender">
                    In this case $f$ is the activation function. The cell output for this
                    <i>
                        time-step
                    </i>
                    is then computed using a third series of parameters $W^{hy}$ : $$y_t = W^{hy}h_t$$
                </p>
                <p>
                    With LSTM or GRU cells, the core principle is the same, but these type of cells additionally use so-called
                    <i>
                        gates
                    </i>
                    that allow to
                    <i>
                        forget
                    </i>
                    information or
                    <i>
                        expose
                    </i>
                    only certain part of the cell state to the next step.
                </p>
                <h3>
                    Sequence to Sequence
                </h3>
                <p class="toRender">
                    Sequence-to-sequence models (or RNN encoder-decoder) have first been introduced in 2014 almost simultaneously by two different research groups
                    <dt-cite key="sutskever2014seq2seq">
                    </dt-cite>
                    <dt-cite key="cho2014encoder-decoder">
                    </dt-cite>
                    . They have recently emerged as the new way-to-go for various generative tasks. The principle of Seq2Seq architecture is to encode information on an input sequence $x_1$ to $x_T$ and to use this condensed representation known as context vector to generate a new sequence $y_1$ to $y_{T^{'}}$. Each output $y_t$ is based on previous outputs and the context vector : $$ p(A|Q) = p(y_1, y_2, ..., y_{T^{'}}|x_1, x_2, ..., x_T) = \prod_{t=1}^{T^{'}}p(y_{t}|c_t, y_1, ..., y_{t-1}) $$
                    The loss $\ell$ is the negative log-probability of the answer sequence $A = [y_1, ..., y_{T^{'}}]$ given the question sequence $Q = [x_1, ..., x_T]$, averaged over the number $N$ of $(Q,A)$ pairs in a minibatch :
                    $$ \ell = -\frac{1}{|N|}\sum_{(Q,A)\in N} p(A|Q) $$
                </p>
                <p>
                    Multiple layers of RNN cells can be stacked over each other to increase the model capacity like in a regular feedfoward neural network.
                </p>
                <p>
                    The following figure presents a Seq2Seq model with a two layers encoder and a two layers decoder:
                    <img class="image" src="images/Seq2SeqDiagram.png">
                </p>
                <p class="toRender">
                    It is also common to have different weights for encoder and decoder cell: $$W^{h*}_{encoder} \neq W^{h*}_{decoder}$$
                </p>
                <h3>
                    Attention Mechanism
                </h3>
                <p class="toRender">
                    Theorically, a large enough and well trained model should be able to perform well on predicting the best decoding sequence given the encoding ones. Indeed, neural networks are universal function approximators
                    <dt-cite key="approximation_function">
                    </dt-cite>
                    , which means they theorically can model any function, even the conditionnal distribution $p(y_1, y_2, ..., y_T|x_1, x_2, ..., x_T)$. However, in practice, we don't have unlimited data, so we need to have a proper inductive bias, that will allow the network to learn to model accurately with a reasonable amout of data.
                </p>
                <p>
                    The first bottleneck with the previous model is that all the relevant information to decode is ed through a fixed size vector. When the encoding sequence is long, it often fails to capture entirely the complex semantic relations between words.
                    .    
                    On the other hand, if the fixed size vector is too large for the encoding sequence, this may cause overfitting problem forcing the encoder to learn some noise.
                </p>
                <p>
                    Furthermore words occuring at the beginning of the encoding sentence may contain information for predicting the first decoding outputs. It is often complex for the model to capture long term dependencies, there is no guarantee that it will learn to handle these properly. This problem can be partially solved by using a Bi-Directionnal RNN (BD-RNN) as the encoder. A BD-RNN will encode its input twice by passing over the input in both directions:
                </p>
                <p>
                    <img class="image" src="images/bidirectionnal-rnn.png" width="650">
                </p>
                <p>
                    This allows for hidden representations at each timestep to capture both future and past context information from the input sequence.  
                </p>
                <p>
                    However, to better overcome the information bottleneck and long-term dependencies limitations, attentions model have been introduced. The basic idea of attention is that instead of attempting to learn a single vector representation for each sentence, we keep around vectors for every word in the input sentence, and reference these vectors at each decoding step.
                </p>
                <ul>
                    <li>
                        <p class="toRender">
                            Outputs from the encoder cell are usually used as attention vectors. We will call them $h_{j}\text{, } \forall j \in [1, T]$. 
                            From the original paper
                            <dt-cite key="attention_mechanism">
                            </dt-cite>
                            equation, a
                            <i>
                                1-layer MLP
                            </i>
                            is used to learn a combination of the decoder previous hidden vector $s_{t-1}$ and all the attention vector output $h_{j=1,...,T}$.
                            $$b_{tj} = v_{a}^{T}tanh(W_{a}s_{t-1} + U_{a}h_{j})$$
                            The optimization algorithm should learn to produce large $b_{tj}$ when the $j$th attention vector is relevant for decoding the $t$th token.
                        </p>
                    </li>
                    <li>
                        <p class="toRender">
                            The $1D$ vector $b_{t} = (b_{t0}, ... ,b_{tT})^T$ is then passed throught a softmax function. One reason to compute the softmax is to keep the math as correct as possible. Removing the softmax may cause values in $b_{t}$ to increase continously with training, while competing against each other to be the most relevant vector to use. Hence, we can see the softmax function as a regularizer.
                            $$\alpha_{tj} = softmax(b_{tj}) = \frac{exp(b_{tj})}{\sum_{k=1}^{T} exp(b_{tk})}$$
                        </p>
                    </li>
                    <li>
                        <p class="toRender">
                            The final context vector is computed as a convex sum of every attention vector:
                            $$ c_t = \sum_{j=1}^{T}\alpha_{tj}h_{j} $$
                            This vector is finally concatenated with the decoder input. The following figure summarizes how the attention mechanism works:
                            <img class="image" src="images/attention_principle.png">
                        </p>
                    </li>
                </ul>
                <h3>
                    Seq2Seq tricks
                </h3>
                <h4>
                    Beam Search
                </h4>
                <p class="toRender">
                    Beam search is a well known heuristic search algorithm, which have been widely adopted in large systems with insufficient amount of memory to store the entire search tree. In the context of sequence generation, and when a model is already trained, the decoder predicts the next token as the $$\arg\!\max_{k \in |K|}(softmax(y_{tk}))$$ where $|K|$ is the decoder vocabulary size. Because the model has been trained with maximum likelihood, the most probable answer is not always the best, and maybe the second best choice of word will condition a better answer. Keeping the $M-$best word prediction at every timestep is what beam search does.
                    <img class="image" src="images/beam_search.jpg">
                </p>
                <h4>
                    Sample softmax
                </h4>
                <p class="toRender">
                    During training, the predicting token $y_k$ is computed with the softmax function, and as |K| is usually larger than the RNN cell hidden size |h|, a dictionnary matrix $W \in \mathbb{R}^{|K|, |h|}$ is constructed. Every row $w_k$ becomes the learned representation of a token $y_k$. <br>
                    In essence, the softmax forces the learning algorithm to orientate row vector $k$ of $W$ in the same direction of the output of the cell when $k$ should be the predicted token, so that the future softmax function will return a larger value at index $k$. <br>
                    However, as |K| becomes large, computation is a bottleneck. A strategy, introduced by
                    <dt-cite key="sample_softmax">
                    </dt-cite>
                    , proposes to alleviate this problem by selecting a small subset of target vocabulary.
                </p>
                <p>
                    Remember that for :
                    </p><figure id="iteratespluserror" style="width:610px; height:105px; display:block; margin-left:auto; margin-right:auto; position:relative">
                        <div style="position:relative; top:-35px">
                            <span class="toRender" style="position:absolute; left:0px; top:0px">
                                $$p(y_t = y_k|y_{&lt; t}, x)$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:00px; top:105px; width:120px;">
                                The probability that the next target words is $y_k$
                            </figcaption>
                            <span class="toRender" style="position:absolute; left:170px; top:0px">
                                $$=$$
                            </span>
                            <span class="toRender" style="position:absolute; left:200px; top:0px">
                                $$e^{w_k^Tg(y_{t-1}, h_t, c_t)}$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:200px; top:105px;width:210px;">
                                Where, $g(y_{t-1}, h_t, c_t) = W^{hy}h_t$,
                                $h_t = W^{hx}x_t + W^{hh}h_{t-1}$, and $c_t$ is the attention vector
                            </figcaption>
                            <span class="toRender" style="position:absolute; left:400px; top:0px">
                                $$/$$
                            </span>
                            <span class="toRender" style="position:absolute; left:440px; top:-10px">
                                $$\displaystyle{\sum_{k^{'} \in K} e^{w_{k^{'}}^Tf(y_{t-1}, h_t, c_t)}}$$
                            </span>
                            <figcaption class="toRender" style="position:absolute; left:440px; top:105px;width:200px;">
                                The sum of all $|K|$ logits.
                            </figcaption>
                        </div>
                    </figure>
                <p></p>
                <p class="toRender">
                    
                </p>

                <p>
                    The main idea of the proposed approach is to approximate the second term of the gradient (i.e 
                    <dt-fn>
                    For simplicity, let's define $\chi(y_k) = w_k^Tf(y_{t-1}, z_t, c_t) + b_k$. <br>
                    Computing the gradient of $log(p(y_t = y_k|y_{&lt; t}, x))$ gives us $ \nabla(\chi(y_k)) - E_p(\nabla(\chi(y)))$ where the first term is  A sparse vector with $1$ non zero entry at row $k$, and the second is a complicated sum $\displaystyle{\sum_{k^{'}}\frac{\chi(y_{k^{'}})}{\sum_{k^{''}}\chi(y_{k^{''}})}\nabla(\chi(y_{k^{'}}))}$.
                    </dt-fn>)
                    , by importance sampling <dt-fn>In a nutshell, importance sampling is a technique to approximate any distribution properties, by sampling from another, usually simpler, distribution.</dt-fn> using a smaller number of target words.<dt-fn>
                                        In their paper
                    <dt-cite key="sample_softmax">
                    </dt-cite>
                    ,
                    <i>
                        Jean et al.
                    </i>
                    approximate the expectation with importance sampling by defining a set of distributions $Q_i$ with their respective subset $K_i$: <br>
                    $E_p(\nabla(\chi(y))) = \displaystyle{\sum_{k^{'} \in K^{'}, |K^{'}| &lt; |K|}\frac{Q(y_{k^{'}})\chi(y_{k^{'}})}{\sum_{k^{''}}Q(y_{k^{''}})\chi(y_{k^{''}})}\nabla(\chi(y_{k^{'}}))}$. <br>

                    </dt-fn>

                    <br>
                    
                </p>
                <p class="toRender">

                </p>
                <p class="toRender">
                    The subset $K_i$, which contains target words, are computed before training:
                    </p><ul>
                        <li>
                            <p class="toRender">
                            Every training decoding sequence $A_j = [y_1, y_2, ..., y_{T^{'}}]_j$  is assigned to a cluster $K_i$. Hence $[y_{1:T^{'}}]_j \in K_i$. 
                            </p>
                        </li>
                        <li>
                            <p class="toRender">
                            Until $|K_i| &lt; S$ ($S$ is an hyperparameter), new training sentences are appended to the cluster. 
                            </p>
                        </li>
                    </ul>
                <p></p>
                <p class="toRender">
                    <b>
                        Conclusion:
                    </b>
                    Sample softmax is a simple method to compute an approximation of the true softmax when the decoder vocabulary size $K$ is large. Instead of moving the true target word vector $w_k$ in the direction of the $y_t$, while pushing all others word vector away, it only moves a subset of them. This is why we used sample softmax in every model involving word level decoding.
                </p>
                <h4>
                    Non Maximum Likelihood approach
                </h4>
                <p>
                    Maximum Likelihood suffers from predicting the most probable answer. It lacks some belief concerning appearance of token in the vocabulary, hence a model trained with maximum likelihood will tend to ouput short general answers that are very common in the vocabulary, like <i>yes</i>, <i>no</i>, <i>maybe</i>. This is a huge problem for conversation diversity. Different approaches have been proposed as a remedy for this issue.
                </p>
                <p>
                    <b><small>Mutual Information Loss</small> </b>
                </p>
                <p class="toRender">
                    One approach was to use Maximum Mutual Information
                    <dt-cite key="diversity_objective_function">
                    </dt-cite>
                    $$\displaystyle{\hat{A} = \arg\!\max_Q\text{ }log\frac{p(A,Q)}{p(A)p(Q)}},$$
                    <br>
                    which measure the mutual dependence between inputs (i.e Q) and the output (i.e A), instead of Maximum Likelihood
                    <br>
                    $$\displaystyle{\hat{A} = \arg\!\max_Q\text{ }log(p(A|Q))}$$
                    From this we can derive a nicer form for the maximization problem $$\arg\!\max\text{ }log(p(A|Q) - \lambda log(p(A))).$$ As we want to control how we should penalize generic answers, a $\lambda$ is added before the second term. Price-Bayes theorem also tells us that $$\displaystyle{\hat{A} = \arg\!\max_Q\text{ } (1 - \lambda)log p(A|Q) + \lambda log p(Q|A)}.$$ This is still unsatisfiable because both penalizer term are nontrivial to use:
                    </p><ul>
                        <li>
                            <p class="toRender">
                                $p(A)$ penalizer leads to ungrammatical responses. To overcome this issue, $p(A) = \prod_{t=1}^{T^{'}}p(y_t|c_t, y_1, y_2, ..., y_{t-1})$ is approximated by multiplying probability at every timestep with an explonential decay $g(t)$. The reason for this is that first words to be predicted are determinant for the remainder of the sentence. Hence penalizing words early in the sentence encourage diversity in the predicted output sequence.
                            </p>
                        </li>
                        <li>
                            <p class="toRender">
                                $p(Q|A)$ is intractable while decoding. One strategy employ in Li et al.
                                <dt-cite key="second_seq2seq">
                                </dt-cite>
                                was to employ a second seq2seq model which predict $p(Q|A)$. First a classic seq2seq predicts the conditionnal probability $p(A|Q)$ using beam search. Then all $M$ prediction are passed as input to the inverse seq2seq, and the loss function is added as the penalizer term.
                            </p>
                        </li>
                    </ul>
                <p></p>
                <p>
                </p><p>
                    <b><small>Adversarial Loss</small> </b>
                </p>
                <p class="toRender">
                    Another elegant way to force more diverse and grammatically correct answers would be to use Generative Adversarial Networks (GANs) <dt-cite key="goodfellow2014generative"></dt-cite>. In this case, having a discriminator conditioned as well on the question sequence $Q$ and classifying output sequences as being real or fake should be able to detect common generic answers that the generator tends to produce. The vanilla GAN objectives can be thought of as a minimax game played between the Generator ($G$) and the Discriminator ($D$) with the dueling objectives : $\min_{G}\max_{D} V(D,G)$, where</p>
                <p class="toRender" align="center">$V(D,G) = E_{x \sim p_{data}(x)}[\log D(x)] + E_{z \sim p_{z}(z)}[\log(1-D(G(z)))]$</p> 
                <p class="toRender">
                In this case, $x$ is a sample of the real data distribution ($p_{data}$), and $z$ is usually noise drawn from a known distribution ($p_z$) easy to sample from (Gaussian or uniform). When training is succesfull, $G$ will have learned to map any noise sample $z$ to "real-looking" samples $G(z)$.
                </p>
                <p class="toRender">
                In our case however, our generator could be the decoder from our <i>seq2seq</i> model, and would therefore learn to map sentence encodings to real data samples. This would yield that $z$ from the GAN formulation would be replaced by $Enc_G(Q)$ where $Enc_G$ is the encoding function of the <i>seq2seq</i>. Our discriminator, in order to judge asnwers according to the questions and not just as real-looking samples, should  also be conditioned on $Q$. In this case a possible strategy would be to provide the discriminator another sentence encoder $Enc_D$ in order to provide a fixed sentence representation to an  RNN performing per time-steps predictions on either real answers our fake ones produced by $G$. This way, the discriminator RNN could output predictions, on each timestep, with respect to (1) its previous state and (2) the global representation of $Q$, producing direct learning signals for the generator at every timestep. Our minimax game value would then look like this :
                    </p><p class="toRender" align="left">$V(D,G) = E_{Q \sim p_{data}(Q)}[\log D(A|Q)]$</p>
                    <p class="toRender" align="right">$ + E_{Q \sim p_{data}(Q)}[\log(1-D(G(Enc_G(Q))))]$</p> 
                <p></p>    
                <p>
                    <b><small>Discrete outputs and REINFORCE Algorithm</small></b>
                </p>
                <p class="toRender">
                Although using GANs for dialogue generation is appealing, one major obstacle would prevent us from applying the idea presented above : the fact that ouput sequences from the generator, as well as true answer sequences are sequences of <i>discrete</i> tokens. Therefore, gradients from the discrminator prediction errors, could not get propageted back passed the $argmax$ operation over the outputted probability distribution of symbols in the generator at each timestep.                    
                </p>
                <p class="toRender">
                Luckily, methods from the field reinforcement learning offer ways to learn from reward signals despite the choice of discrete actions. Specifically, if we consider the $softmax$ output of $G$ as being a policy $\pi$ for  discrete actions (word or char tokens) given the state ($S_t$, given by the hidden state of the decoder), on could frame this problem as a reinforcement learning one, en employ a policy-gradient technique, such as the REINFORCE algorithm<dt-cite key="williams1992simple"> </dt-cite>. This approach have been applied succesfully to text generation <dt-cite key="yu2016seqgan"> </dt-cite>. In our case however, using an RNN discriminator, outputing rewards at each timestep could potentialy remove the need for Monte-Carlo search, as we could trivialy compute real, complete returns as sums of reward from each $t \in T^{'}$. Moreover, adding conditioning on the question representation would allow for <i>answer</i> generation instead of basic text generation.   
                </p>        
                <h2>Experiments and results</h2>
                    <h3>Network Architectures</h3>
                        <p>We performed experiments with multiple variants of the sequence-to-sequence architecture described above. As sequence to sequence model offer loosely coupled encoder and decoder networks, one can easily change input or output data formats without changing the architecture. Of course, re-training is then needed. In our case, we experimented with different language abstraction levels for both the encoder and decoder. We compared word-level and charcater-level representations of sentences as input or ouput formats. We experimented with 3 variants: <i>word2word</i>, <i>word2char</i>, and <i>char2char</i>, where for example <i>word2char</i> represents a word-level encoder coupled with a char-level decoder.</p>
                        <p>In all cases, for encoder and decoder networks, we used 3 stacked GRU hidden layers of each 256 cells. Feed-forward dropout, when applied, was used with a probability of 0.25 of dropping units, and the Adam optimizer was used to minimize the negative log-likelihood. All experiments were ran on NVIDIA GeForce 960M GPUs.</p>
                    <h3>Data</h3> 
                        <p>We focused our empirical study on a movie-dialogue dataset, while experimenting also on some custom dataset of Facebook Messenger conversations, retrieved from the author's own history of conversations.</p>
                        <h4>Cornell Movie Dialog Corpus</h4>
                            <p>This dataset contains movie conversations extracted from scripts of 617 movies, between more than 10 000 caracters. We extract first Q/A pairs of each conversation and retrieve more than 120 000 samples. We apply 2 preprocessing strategies in order to extract the 8000 most common words, or our arbitrary vocabulary of 55 characters. Most sentences contain common, well written words and we therefore consider this as our <i>clean</i> dataset.</p>
                        <h4>Messenger Conversations</h4>
                            <p>As it is possible for one to retrieve his/her own Facebook conversations' history, the authors wanted to experiment with this data and find out if a trained <i>seq2seq</i> on this data could reflect one's style of written dialogue. However, some serious limitations of the data made it hard for a network to output answers strongly linked semanticaly to the input query. First, the data is limited, and unbalanced given different authors, as shown in the Table below : <img class="meme" src="images/MessengerStatsNoTrebble.png" ,="" width="650"> Secondly, as humans often do on Facebook, the authors do not always have a <i>clean</i> or proper spelling when on Facebook, compared to when writing more official documents, or compared with the movie dialogue corpus. Moreover, a network trained on the <i>clean</i> movie data, could not be used as a pre-trained model for Messenger as this dataset contains only english sentences, while authors are all french speakers. For these reasons, experiments done with these datasets were all done on a character-level encoders and decoders.</p>
                    <h3>Results</h3>
                        <h4>Cornell Movie Dialog Corpus</h4>
                            <p><small><b>Char2Char, no attention, no dropout</b></small></p>
                            <p>Without Droupout, learning curves produced while training look like this for most models: 
                            </p><p><figure>
                                 <img class="meme" src="images/char2char_noAtt.png" ,="" width="650">
                                <figcaption style="text-align: center; bottom: 0px;">Learning curves for the word2word architecture.</figcaption>
                              </figure></p><p>
                            Each epoch consisted of 250 iterations in which a random minibatch is fed to the network. The labels present on the Figure are typical outputs of the network at different training phases. One common behavior in our experiments was the way networks started of being very boring on the validation set, and evovled to being somewhat entertaining on the validation set, despite having bigger loss. This is directly linked to the negative log-likelihood loss function. Therefore, overfitting in case where we aim at entertaining dialogues with little training data might not be a bad thing. Learning some answers, that are grammatically correct from the training set has at least the advantage of creating less abiguous answers than <i>"I don't know"</i> or <i>"Yes"</i>.</p>
                            <p><small><b>Adding attention and dropout.</b></small></p>
                            <p>In order to improve quality of generated answers, we added the attention mechanism described above, so that it can be easier for the network to use specific words from the question query as conditioning information. Also, we added feed-froward dropout in order to help generalization of the model. We list here training curves and sample answers for fixed, custom questions for different models, and discuss these results in the Discussion section. When using word-level decoders, we used the <i>sample softmax</i> as described above.</p>
                            <p><small><u>Word2Word</u></small></p>
                            <p></p>
                              <figure>
                                <img src="images/curves_word2word.png", width="650">
                                <figcaption style="text-align: center;">Learning curves for the <i>word2word</i> architecture with attention.</figcaption>
                              </figure><p></p>
                              <figure>
                                <center><img src="images/samples_w2w.png", style="width: 90%; height: 90%; align: center;"></center>
                                <figcaption style="text-align: center;">Samples for the <i>word2word</i> architecture with attention.</figcaption>
                              </figure><p></p>
                            <p><small><u>Char2Char</u></small></p>
                            <p><figure>
                                 <img class="meme" src="images/curves_char2char_Att_Drop.png" ,="" width="650">
                                <figcaption style="text-align: center; bottom: 0px;", text-align: center>Learning curves for the <i>char2char</i> architecture with attention.</figcaption>
                              </figure></p>
                              <figure>
                                  <center><img src="images/samples_c2c.png", style="width: 75%; height: 75%; align: center;"></center>
                                <figcaption style="text-align: center;">Samples for the <i>word2word</i> architecture with attention.</figcaption>
                              </figure><p></p>
                            <p><small><u>Word2Char</u></small></p>
                            <p><figure>
                                 <img class="meme" src="images/curves_word2char_att_drop.png" ,="" width="650">
                                <figcaption style="text-align: center; bottom: 0px;", text-align: center>Learning curves for the <i>word2char</i> architecture with attention.</figcaption>
                              </figure></p>
                              <figure>
                                  <center><img src="images/samples_w2c.png", style="width: 75%; height: 75%; align: center;"></center>
                                <figcaption style="text-align: center;">Samples for the <i>word2char</i> architecture with attention.</figcaption>
                              </figure><p></p>
                            <p><h4>Messenger</h4></p>
                        <p><small><b>Capturing Styles</b></small></p>
                        <p><small><b>Interactive ChatBot</b></small></p>
                    <h2>Self-teaching method</h2>
                    <p>Before beginning this project, only one member of the team already had experience with RNNs. In order to reach some common level of understanding, we read several blogs, papers and tutorials to familiarize ourselves with seq2seq architectures and several techniques often used in text generation like attention mechanism and sample-softmax. In particular, assisting to a lecture of the Deep Learning course at UdeM really helped us grasp the fundamentals basics of the attention mechanism. We then started coding our own implementation inspired by other work found on the internet. This kind of self-teaching approach is well suited for exploratory projects as it allows us to discover the different methods at our own rate and also make us run into problems to which we have to find the solutions by ourselves.</p>

                    <h2>Conclusion</h2>
                    <p>We implemented a Seq2Seq model for short text generation. The model has been trained of two different datasets, the Cornell Movie Questions and Answers and the author's Facebook Chat histories. For the Cornell dataset, three variations of the model have been compared : char2char, word2word and word2char. The three models were qualitatively comparable but word2char might provide slightly more sophisticated answers. For the Facebook dataset, only char2char have been implemented due to the unguaranteed grammatical correctness of the data. The model have proven able to capture each author's style, with especially visible variations between the French and Quebecer styles. Finally, with more time, we would have liked to improve the attention mechanism results and to implement the GAN approach that uses a discriminator to make our generator reply more naturally.</p> 
            
        
        <dt-appendix class="centered">
            </dt-appendix></p><h3>Acknowledgments</h3>
            <p>Thank you</p>
        
        <script type="text/javascript">
        var el = document.getElementsByClassName("toRender")
        for (var i = 0; i < el.length; i++) {
            renderMath(el[i]);
        }
        </script>
        <script type="text/bibliography">
 @article{bengio1994difficult, title={Learning long-term dependencies with gradient descent is difficult}, author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo}, journal={IEEE transactions on neural networks}, volume={5}, number={2}, pages={157--166}, year={1994}, publisher={IEEE} }

 @article{Chung2014GRUvsLSTM, author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio}, title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, journal = {CoRR}, volume = {abs/1412.3555}, year = {2014}, url = {http://arxiv.org/abs/1412.3555}, timestamp = {Thu, 01 Jan 2015 19:51:08 +0100}, biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChungGCB14}, bibsource = {dblp computer science bibliography, http://dblp.org} }
 @article{shang2015shortText, title={Neural responding machine for short-text conversation}, author={Shang, Lifeng and Lu, Zhengdong and Li, Hang}, journal={arXiv preprint arXiv:1503.02364}, year={2015} } @article{vinyals2015conversational, title={A neural conversational model}, author={Vinyals, Oriol and Le, Quoc}, journal={arXiv preprint arXiv:1506.05869}, year={2015} }; @article{Rush2015summarization, author = {Alexander M. Rush and Sumit Chopra and Jason Weston}, title = {A Neural Attention Model for Abstractive Sentence Summarization}, journal = {CoRR}, volume = {abs/1509.00685}, year = {2015}, url = {http://arxiv.org/abs/1509.00685}, timestamp = {Thu, 01 Oct 2015 14:28:48 +0200}, biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/RushCW15}, bibsource = {dblp computer science bibliography, http://dblp.org} }; @article{Bahdanau2014translation, author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio}, title = {Neural Machine Translation by Jointly Learning to Align and Translate}, journal = {CoRR}, volume = {abs/1409.0473}, year = {2014}, url = {http://arxiv.org/abs/1409.0473}, timestamp = {Wed, 01 Oct 2014 15:00:04 +0200}, biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/BahdanauCB14}, bibsource = {dblp computer science bibliography, http://dblp.org} } @InProceedings{CornellDataset, author={Cristian Danescu-Niculescu-Mizil and Lillian Lee}, title={Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.}, booktitle={Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011}, year={2011} } @inproceedings{sutskever2014seq2seq, title={Sequence to sequence learning with neural networks}, author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V}, booktitle={Advances in neural information processing systems}, pages={3104--3112}, year={2014} } ; @article{cho2014encoder-decoder, title={Learning phrase representations using RNN encoder-decoder for statistical machine translation}, author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua}, journal={arXiv preprint arXiv:1406.1078}, year={2014} }; @article{breathnach1977ovalbumin, title={Ovalbumin gene is split in chicken DNA}, author={Breathnach, Rory and Mandel, Jean-Louis and Chambon, Pierre}, journal={Nature}, volume={270}, pages={314--319}, year={1977} }
     @article{approximation_function, title={Multilayer Feedforward Networks are Universal Approximators}, author={Hornik, StinchXombe, White}, journal={Neural 
Networks}, volume={2}, pages={359--366}, year={1989}}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{yu2016seqgan,
  title={Seqgan: sequence generative adversarial nets with policy gradient},
  author={Yu, Lantao and Zhang, Weinan and Wang, Jun and Yu, Yong},
  journal={arXiv preprint arXiv:1609.05473},
  year={2016}
}

@article{attention_mechanism,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 01 Oct 2014 15:00:04 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{sample_softmax,
  author    = {S{\'{e}}bastien Jean and
               Kyunghyun Cho and
               Roland Memisevic and
               Yoshua Bengio},
  title     = {On Using Very Large Target Vocabulary for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1412.2007},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.2007},
  timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/JeanCMB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
};
@article{diversity_objective_function,
  author    = {Jiwei Li and
               Michel Galley and
               Chris Brockett and
               Jianfeng Gao and
               Bill Dolan},
  title     = {A Diversity-Promoting Objective Function for Neural Conversation Models},
  journal   = {CoRR},
  volume    = {abs/1510.03055},
  year      = {2015},
  url       = {http://arxiv.org/abs/1510.03055},
  timestamp = {Sun, 01 Nov 2015 17:30:44 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiGBGD15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{second_seq2seq,
  author    = {Jiwei Li and
               Dan Jurafsky},
  title     = {Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1601.00372},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.00372},
  timestamp = {Mon, 01 Feb 2016 15:36:05 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiJ16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
        </script>


</dt-article></body></html>